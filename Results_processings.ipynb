{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Toyaa1pKp0T9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the 500 images dataset 50 images at a time\n",
        "# change the filepaths to fit your situation\n",
        "\n",
        "\n",
        "f50 = open('/content/drive/MyDrive/advanced_geo_ai/500_results/small_50.json')\n",
        "data50 = json.load(f50)\n",
        "\n",
        "f100 = open('/content/drive/MyDrive/advanced_geo_ai/500_results/small_100.json')\n",
        "data100 = json.load(f100)\n",
        "\n",
        "f150 = open('/content/drive/MyDrive/advanced_geo_ai/500_results/small_150.json')\n",
        "data150 = json.load(f150)\n",
        "\n",
        "f200 = open('/content/drive/MyDrive/advanced_geo_ai/500_results/small_200.json')\n",
        "data200 = json.load(f200)\n",
        "\n",
        "f250 = open('/content/drive/MyDrive/advanced_geo_ai/500_results/small_250.json')\n",
        "data250 = json.load(f250)\n",
        "\n",
        "f300 = open('/content/drive/MyDrive/advanced_geo_ai/500_results/small_300.json')\n",
        "data300 = json.load(f300)\n",
        "\n",
        "f350 = open('/content/drive/MyDrive/advanced_geo_ai/500_results/small_350.json')\n",
        "data350 = json.load(f350)\n",
        "\n",
        "f400 = open('/content/drive/MyDrive/advanced_geo_ai/500_results/small_400.json')\n",
        "data400 = json.load(f400)\n",
        "\n",
        "f450 = open('/content/drive/MyDrive/advanced_geo_ai/500_results/small_450.json')\n",
        "data450 = json.load(f450)\n",
        "\n",
        "f500 = open('/content/drive/MyDrive/advanced_geo_ai/500_results/small_500.json')\n",
        "data500 = json.load(f500)"
      ],
      "metadata": {
        "id": "6_kHCyF4qCEh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Since the GPS Coordinates are contained in the filenames, we use this helper function to extract them out\n",
        "def extract_gps(filename):\n",
        "    parts = filename.split('_')\n",
        "    if len(parts) > 0:\n",
        "        coords = parts[0].split(',')\n",
        "        if len(coords) == 2:\n",
        "            return [coords[0], coords[1]]\n",
        "    return None, None\n",
        "\n"
      ],
      "metadata": {
        "id": "15GxWOCKI8Jz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This function is used to calculate the nature index and urban index for each image, and extract gps coords from filename,\n",
        "## and put everything in one big list so we can convert everything into a pandas dataframe.\n",
        "\n",
        "\n",
        "def calculate_proportions(rs):\n",
        "\n",
        "  results = []\n",
        "\n",
        "  # Specific numbers we're interested in.\n",
        "  #These stands for the following labels {8: 'vegetation',9: 'terrain',10: 'sky',1: 'sidewalk', 18: 'bicycle'}\n",
        "  nature = [8, 9, 10, 1, 18]\n",
        "\n",
        "  for img in rs:\n",
        "\n",
        "    arr = np.array(img['segmentaion'])\n",
        "\n",
        "    arr = arr.flatten()\n",
        "\n",
        "    flattened_arr = arr.flatten()\n",
        "\n",
        "    # Get unique numbers and their counts\n",
        "    unique_numbers, counts = np.unique(flattened_arr, return_counts=True)\n",
        "\n",
        "    # Calculate proportions\n",
        "    proportions = counts / flattened_arr.size\n",
        "\n",
        "    # Create a dictionary to see each number and its proportion\n",
        "    number_proportions = dict(zip(unique_numbers, proportions))\n",
        "\n",
        "\n",
        "    # Filter counts for specific numbers\n",
        "    specific_counts = counts[np.isin(unique_numbers, nature)]\n",
        "\n",
        "    # filter counts for the rest\n",
        "    the_rest_counts = counts[~np.isin(unique_numbers, nature)]\n",
        "\n",
        "    # Calculate the sum of counts for specific numbers\n",
        "    specific_sum = np.sum(specific_counts)\n",
        "\n",
        "    # Calculate the sum of the counts for the rest\n",
        "    the_rest_sum = np.sum(the_rest_counts)\n",
        "\n",
        "    # Total elements in the array\n",
        "    total_elements = flattened_arr.size\n",
        "\n",
        "    # Calculate the proportion of specific numbers\n",
        "    nature_proportion = specific_sum / total_elements\n",
        "\n",
        "    # Calculate the proportion of the rest of the numbers\n",
        "    urban_proportion = the_rest_sum / total_elements\n",
        "\n",
        "    #extract the GPS coords for visualisation later on\n",
        "    coords = extract_gps(img['filename'])\n",
        "\n",
        "    results.append({'coords':coords, 'filename': img['filename'], 'proportions': number_proportions, 'nature':nature_proportion, 'urban': urban_proportion})\n",
        "\n",
        "  return results\n",
        ""
      ],
      "metadata": {
        "id": "ucvglGyPvBH6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "pro50 = calculate_proportions(data50)\n",
        "pro100 = calculate_proportions(data100)\n",
        "pro150 = calculate_proportions(data150)\n",
        "pro200 = calculate_proportions(data200)\n",
        "pro250 = calculate_proportions(data250)\n",
        "pro300 = calculate_proportions(data300)\n",
        "pro350 = calculate_proportions(data350)\n",
        "pro400 = calculate_proportions(data400)\n",
        "pro450 = calculate_proportions(data450)\n",
        "pro500 = calculate_proportions(data500)"
      ],
      "metadata": {
        "id": "2z84xJzkxEEJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This function converts into pandas dataframe\n",
        "def as_pandas(rs):\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      'latitude': [item['coords'][0] for item in rs],\n",
        "      'longitude': [item['coords'][1] for item in rs],\n",
        "      'nature_index': [item['nature'] for item in rs],\n",
        "      'urban_index': [item['urban'] for item in rs]\n",
        "  })\n",
        "\n",
        "  # Convert latitude and longitude to numeric types\n",
        "  df['latitude'] = pd.to_numeric(df['latitude'])\n",
        "  df['longitude'] = pd.to_numeric(df['longitude'])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "4WoWjgmwVRLC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pro50 = as_pandas(pro50)\n",
        "pro100 = as_pandas(pro100)\n",
        "pro150 = as_pandas(pro150)\n",
        "pro200 = as_pandas(pro200)\n",
        "pro250 = as_pandas(pro250)\n",
        "pro300 = as_pandas(pro300)\n",
        "pro350 = as_pandas(pro350)\n",
        "pro400 = as_pandas(pro400)\n",
        "pro450 = as_pandas(pro450)\n",
        "pro500 = as_pandas(pro500)"
      ],
      "metadata": {
        "id": "TPHII9QzZmzO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## concatenate everything into one single pandas dataframe\n",
        "\n",
        "final_ready_vis = pd.concat([pro50, pro100, pro150, pro200, pro250, pro300, pro350, pro400, pro450, pro500], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "h19ULvVuaX3B"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Using pandas 'cut' to create 5 equal-width bins for the 'nature_index'\n",
        "bins, bin_edges = pd.cut(final_ready_vis['nature_index'], bins=5, retbins=True, right=True)\n",
        "\n",
        "# Create labels based on the bin edges\n",
        "bin_labels = [f\"{round(bin_edges[i], 2)} to {round(bin_edges[i+1], 2)}\" for i in range(len(bin_edges)-1)]\n",
        "\n",
        "# Assign these labels to the bins\n",
        "final_ready_vis['nature_index_equal_bin'] = pd.cut(final_ready_vis['nature_index'], bins=5, labels=bin_labels)\n",
        "\n",
        "# Using pandas 'cut' to create 5 equal-width bins for the 'nature_index'\n",
        "bins, bin_edges = pd.cut(final_ready_vis['urban_index'], bins=5, retbins=True, right=True)\n",
        "\n",
        "# Create labels based on the bin edges\n",
        "bin_labels = [f\"{round(bin_edges[i], 2)} to {round(bin_edges[i+1], 2)}\" for i in range(len(bin_edges)-1)]\n",
        "\n",
        "# Assign these labels to the bins\n",
        "final_ready_vis['urban_index_equal_bin'] = pd.cut(final_ready_vis['urban_index'], bins=5, labels=bin_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "cMp-IoeUCI73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_ready_vis.to_csv('my_dataframe.csv', index=False)  # export this csv so we can visualize everything in QGIS\n"
      ],
      "metadata": {
        "id": "ny9FMxEjbXrK"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}